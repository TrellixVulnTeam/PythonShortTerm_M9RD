{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆向工程\n",
    "\n",
    " 根据联合国全球网站可访问性审计报告 ， 73%的主流网站都在其重要功能中依赖JavaScript。和单页面应用的简单表单事件不同 ，使用 JavaScript 时，不再是加载后立即下载所有页面内容 。这样就会造成许多网页在浏览器中展示的内容不会出现在 HTML源代码中，对于这种依赖JavaScript的动态网站， 介绍两种抓取其数据的方法，分别是\n",
    "\n",
    "1、 JavaScript 逆向工程\n",
    "\n",
    "2、 渲染JavaScript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看动态网页示例\n",
    "\n",
    "示例网站有一个搜索表单， 可以通过\n",
    "\n",
    "http://example.webscraping.com/places/default/search\n",
    "\n",
    "进行访问，该页面用于查询国家。比如说，我们想要查找所有起始字母为 A 的国家。\n",
    "\n",
    "如果使用Chrome浏览器，先按F12键，我们单击结果部分， 可以发现结果被存储在ID为 “result” 的div元素中，让我们尝试使用lxml模块抽取这些结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import random\n",
    "import urlparse\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class Downloader:\n",
    "\n",
    "    def __init__(self,delay=5,user_agent=\"Mozilla/5.0\",proxies=None,num_retries=1,cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.proxies = proxies\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "\n",
    "    def __call__(self, url):\n",
    "        '''\n",
    "        在该方法中我们实现了下载前检查缓存的功能。该方法首先会检\n",
    "        查缓存是否已经定义 。如果已经定义 ，则检查之前是否已经缓存了该URL 。\n",
    "        如果该 URL 己被缓存 ， 则检查之前的下载中是否遇到了服务端错误 。\n",
    "        最后 ，如果也没有发生过服务端错误，则表明该缓存结果可用\n",
    "        '''\n",
    "\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                # 地址不在缓存中\n",
    "                pass\n",
    "            else:\n",
    "                # 如果之前服务器出现错误，忽略缓存中的数据，重新下载\n",
    "                if self.num_retries >0 and 500 <= result['code'] < 600:\n",
    "                    result = None\n",
    "\n",
    "        # 结果没有从缓存中加载，所以要重新下载\n",
    "        if result is None:\n",
    "            self.throttle.wait(url)\n",
    "            proxy = random.choice(self.proxies) if self.proxies else None\n",
    "            header = {'User-agent':self.user_agent}\n",
    "            result = self.download(url,header,proxy,self.num_retries)\n",
    "\n",
    "            if self.cache:\n",
    "                # 保存结果缓存\n",
    "                self.cache[url]=result\n",
    "\n",
    "        return result['html']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def download(self,url, headers, proxy, num_retries, data=None):\n",
    "        '''\n",
    "        download 方法和之前的 download 函数基本一样 ，只是在返回下载\n",
    "        的 HTML 时额外返回了HTTP 状态码,以便在缓存中存储错误码。当然，如\n",
    "        果你只需要一个简单的下载功能,而不需要限速或缓存的话，可以直接调用\n",
    "        该方法,这样就不会通过call方法调用了 。\n",
    "        :param url:\n",
    "        :param headers:\n",
    "        :param proxy:\n",
    "        :param num_retries:\n",
    "        :param data:\n",
    "        :return:\n",
    "        '''\n",
    "        print 'Downloading:', url\n",
    "        request = urllib2.Request(url, data, headers)\n",
    "        opener = None\n",
    "        if proxy:\n",
    "            proxy_handler = urllib2.ProxyHandler({\"http\" : \"http://%(host)s:%(port)d\" % proxy})\n",
    "            opener = urllib2.build_opener(proxy_handler)\n",
    "        else:\n",
    "            opener = urllib2.build_opener()\n",
    "        try:\n",
    "            response = opener.open(request)\n",
    "            html = response.read()\n",
    "            code = response.code\n",
    "        except urllib2.URLError as e:\n",
    "            print 'Download error:', e.reason\n",
    "            html = ''\n",
    "            if hasattr(e, 'code'):\n",
    "                code = e.code\n",
    "                if num_retries > 0 and 500 <= code < 600:\n",
    "\n",
    "                    return self.download(url, headers, proxy, num_retries-1, data)\n",
    "            else:\n",
    "                code = None\n",
    "\n",
    "        return {'html':html,'code':code}\n",
    "\n",
    "class Throttle:\n",
    "    \"\"\"设置下载速度\n",
    "    \"\"\"\n",
    "    def __init__(self, delay):\n",
    "\n",
    "        self.delay = delay\n",
    "\n",
    "        self.domains = {}\n",
    "\n",
    "    def wait(self, url):\n",
    "        domain = urlparse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()\n",
    "\n",
    "import lxml.html\n",
    "\n",
    "D = Downloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/search\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "html = D('http://example.webscraping.com/places/default/search')\n",
    "tree = lxml.html.fromstring(html)\n",
    "print tree.cssselect('div#results a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么也没有"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
