{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID遍历爬虫"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们将利用网站结构的弱点,更加轻松地访问所有内容\n",
    "\n",
    "• http://example.webscraping.com/view/Afghanistan-1\n",
    "• http://example.webscraping.com/view/Australia-2\n",
    "• http://example.webscraping.com/view/Brazil-3\n",
    "\n",
    "可以看出，这些URL只在结尾处有所区别，包括国家名（作为页面别名 ）和ID。\n",
    "在URL中包含页面别名是非常普遍的做法 ，可以对搜索引擎优化起到\n",
    "帮助作用。 一般情况 下， Web 服务器会忽略这个字符串，只使用 ID 来匹配数\n",
    "据库中的相关记录 。\n",
    "下面我们将其移除 ，加载 http://example.webscraping.com/view/1，测试示例网站中的链接是否仍然可用 。\n",
    "网页依然可以加载成功，也就是说该方法是有用的,我们就可以忽略页面别名， 只遍历ID来下载所有国家的页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  urllib\n",
    "def download(url, user_agent='Mozilla/5.0' ,num_retries=2 ):\n",
    "    print(\"Downloading!\",url)\n",
    "    headers = {\"User-agent\":user_agent}\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "    try:\n",
    "        html=urllib.request.urlopen(request).read()\n",
    "    except urllib.request.URLError as e:\n",
    "        print (\"url error:\",e.reason)\n",
    "        html=None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "                return download(url,user_agent,num_retries-1)\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "# *************************更新区域*************************\n",
    "# **********************************************************\n",
    "import itertools\n",
    "def traverse_site_v1(url):\n",
    "    for page in itertools.count(1):\n",
    "\n",
    "        html = download(url+ (\"%d\" % page))\n",
    "        if html is None:\n",
    "            break\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现的爬虫需要连续5 次下载错误才会停止遍历，这样就很大程度上降低了遇到被删除记录时过早停止遍历的风险"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_site_v2(url):\n",
    "    max_error=5\n",
    "    num_error=0\n",
    "    for page in itertools.count(1):\n",
    "\n",
    "        html = download(url+ (\"%d\" % page))\n",
    "        # print(html)\n",
    "        if html is None:\n",
    "            num_error += 1\n",
    "\n",
    "            if num_error==max_error:\n",
    "                break\n",
    "        else:\n",
    "            num_error=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_site_v2(\"http://example.webscraping.com/places/default/view/\")\n",
    "# traverse_site_v2(\"http://example.webscraping.com/view/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "遍历ID 是一个很便捷的方法，但是和网站地图爬虫一样，这种方法也无法保证始终可用。 \n",
    "\n",
    "比如，一些网站会检查页面别名是否满足预期，如果不是，则会返回404 Not Found 错误。\n",
    "而另一些网站则会使用非连续大数作为ID，或是不使用数值作为ID， 此时遍历就难以发挥其作用了。\n",
    "\n",
    "例如， Amazon使用ISBN作为图书ID， 这种编码包含至少 10 位数字。\n",
    "使用ID对 Amazon的图书进行遍历需要测试数十亿次， 因此这种方法肯定不是抓取该站内容最高效的方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
